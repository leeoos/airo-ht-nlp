{"cells":[{"cell_type":"markdown","metadata":{"id":"iF2Qs5dexd_6"},"source":["# Homework 1"]},{"cell_type":"markdown","metadata":{"id":"5-9yYVjSxFdv"},"source":["## Load libraries, stopwords, link google drive and set up some utils fnctions"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GjTeQ_xTq_No","executionInfo":{"status":"ok","timestamp":1704466533118,"user_tz":-60,"elapsed":58766,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"}},"outputId":"515c2b06-91ef-4348-a190-4afc6283c366"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# Insatll packages\n","!pip install -q wget # to download data\n","!pip install -q spacy\n","!python -m spacy download en_core_web_sm > /dev/null 2>&1"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18326,"status":"ok","timestamp":1704466551434,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"},"user_tz":-60},"id":"sd4GzYfDrF3W","outputId":"dab7b483-61fb-4537-d201-6130606549fb"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]}],"source":["# Import library\n","%matplotlib inline\n","import numpy as np\n","import gensim\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import wget\n","import spacy\n","import scipy.stats\n","\n","from tqdm import tqdm\n","from nltk.corpus import stopwords\n","import nltk\n","import re\n","from collections import defaultdict\n","\n","from IPython.display import display, HTML\n","import zipfile\n","import json\n","import time\n","import os\n","import math\n","\n","import gdown\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.corpus import wordnet as wn\n","from nltk.stem import WordNetLemmatizer"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1704466551434,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"},"user_tz":-60},"id":"SqKBoP_u7sxG","outputId":"d275bb4d-6735-4d1a-fca1-189859990513"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["# Stopwords\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"GTHQMdeZrexV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704466575462,"user_tz":-60,"elapsed":24037,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"}},"outputId":"690b4220-8f9d-4bed-f2d8-255b2de095dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Connect google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"4qf-HTe1kYXj","executionInfo":{"status":"ok","timestamp":1704466575463,"user_tz":-60,"elapsed":7,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"}}},"outputs":[],"source":["# Utils\n","def custom_unzip(zip_file, PATH):\n","  with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n","    for member in tqdm(zip_ref.infolist(), desc='Extracting '):\n","      try:\n","        zip_ref.extract(member, PATH)\n","\n","      except zipfile.error as e:\n","        pass\n","\n","def get_data_from_user():\n","  print(\"Please get data from: \")\n","  link = {\n","      'text': 'sense embedding',\n","      'url': 'https://drive.google.com/file/d/1JZmM8--5By4aN0HoPbYaMMfHuYJyibyb/view?usp=sharing'\n","  }\n","  html_link = f'<a href=\"{link[\"url\"]}\" target=\"_blank\">{link[\"text\"]}</a>'\n","  display(HTML(html_link))\n","  print(\"and save it in: /content/drive/MyDrive/\")\n","\n","  trial = 0;\n","  while(trial < 10000000):\n","    time.sleep(1)\n","\n","    folder = '/content/drive/MyDrive/sense-embeddings'\n","    if os.path.exists(folder):\n","      print(\"data acquired\")\n","      break\n","\n","    trial += 1\n","    if trial == 10000000:\n","      print(\"time exeeded\")\n","      break\n","\n","  return folder\n","\n","def get_wordnet_pos(word):\n","  \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n","  tag = word[1][0].upper()\n","  tag_dict = {\"J\": wn.ADJ,\n","              \"N\": wn.NOUN,\n","              \"V\": wn.VERB,\n","              \"R\": wn.ADV}\n","  return tag_dict.get(tag, wn.NOUN)\n","\n","def backup_preprocessed_data(data, destination):\n","  with open(destination, 'w') as file:\n","    file.writelines(' '.join(row) + '\\n' for row in data)"]},{"cell_type":"markdown","metadata":{"id":"dfkF_w-axJmo"},"source":["## Get MOSAICo and Semantic Simlex999"]},{"cell_type":"markdown","metadata":{"id":"DMtk_-XHe45I"},"source":["### MOSAICo"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"nB9QQtheT_Z0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704466774557,"user_tz":-60,"elapsed":4250,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"}},"outputId":"a41a9791-d88e-4887-e053-d5fa0caa68dc"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1HiAj6q37Wu6yScq9dkydVLVNHv1JEciG\n","To: /content/500000.jsonl\n","100%|██████████| 213M/213M [00:02<00:00, 106MB/s] \n","Downloading...\n","From: https://drive.google.com/uc?id=15TX6LudgvOCiMb0v5vJY6x6HM8sUzG0C\n","To: /content/semantic_simlex_v0.1.tsv\n","100%|██████████| 81.3k/81.3k [00:00<00:00, 19.4MB/s]\n"]}],"source":["# Get dataset\n","# Check if the dataset is on drive, if not ask the user to download it\n","mosaico = '/content/drive/MyDrive/AI_Robotics/NLP/sense-datasets/500000.jsonl'\n","is_here_mosaico = os.path.exists(mosaico)\n","\n","simlex = '/content/drive/MyDrive/AI_Robotics/NLP/sense-datasets/semantic_simlex_v0.1.tsv'\n","is_here_simlex = os.path.exists(simlex)\n","\n","external_download = False\n","\n","if not is_here_mosaico or not is_here_simlex :\n","\n","  if external_download:\n","    folder = get_data_from_user()\n","\n","    # unzip the needed dataset\n","    if not is_here_mosaico:\n","      custom_unzip(folder + '/sample_annotated_sentences.zip', '/content/')\n","      mosaico = '/content/500000.jsonl'\n","\n","    if not is_here_simlex:\n","      custom_unzip(folder + '/semantic_simlex_v0.1.zip', '/content/')\n","      simlex = '/content/semantic_simlex_v0.1.tsv'\n","\n","  else:\n","    file_id = '1HiAj6q37Wu6yScq9dkydVLVNHv1JEciG'\n","    url = f\"https://drive.google.com/uc?id={file_id}\"\n","    mosaico = '/content/500000.jsonl'\n","    gdown.download(url, mosaico, quiet=False)\n","\n","    file_id = '15TX6LudgvOCiMb0v5vJY6x6HM8sUzG0C'\n","    url = f\"https://drive.google.com/uc?id={file_id}\"\n","    simlex = '/content/semantic_simlex_v0.1.tsv'\n","    gdown.download(url, simlex, quiet=False)\n","\n","else:\n","  print(\"data acquired\")\n"]},{"cell_type":"markdown","metadata":{"id":"U49GmfvPeacu"},"source":["### Simlex999"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVmSaMdUevPE"},"outputs":[],"source":["# Extract simlex data\n","simplex_pairs = dict()\n","simlex_sense_pairs = dict()\n","with open(simlex, 'r') as simlex_file:\n","  next(simlex_file)\n","\n","  for line in simlex_file:\n","    splitted_line = line.strip().split()\n","\n","    w1, w2, pos, score, *_ = splitted_line\n","    simplex_pairs[(w1, w2)] = float(score)\n","\n","    s1, s2 = splitted_line[-2], splitted_line[-1]\n","    # for sp1 in s1.split(','):\n","    #   for sp2 in s2.split(','):\n","    #     simlex_sense_pairs[(sp1, sp2)] = float(score)\n","    simlex_sense_pairs[(s1, s2)] = float(score)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b_nKetNSewG-"},"outputs":[],"source":["simplex_pairs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GgY9oaZYhpzn"},"outputs":[],"source":["simlex_sense_pairs"]},{"cell_type":"markdown","metadata":{"id":"PzjkEW3Iqr4i"},"source":["## Word Embedding"]},{"cell_type":"markdown","metadata":{"id":"pDQdtm5lxqvH"},"source":["### Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FUKAVgK1oQLZ"},"outputs":[],"source":["# Standard functions to process text\n","# nlp = spacy.load(\"en_core_web_sm\")\n","regexp_alphbetic = re.compile('[^a-zA-Z]+')\n","lemmatizer = WordNetLemmatizer()\n","\n","def preprocess_word_text(sentence, stopwords, lemmatize=True):\n","  # doc = nlp(sentence)\n","  doc = sentence.split(' ')\n","  pos = nltk.pos_tag(doc)\n","  sentence_tokens = []\n","\n","  for token_id, token in enumerate(doc):\n","    token_text = lemmatizer.lemmatize(token, get_wordnet_pos(pos[token_id])) if lemmatize else token.text\n","    token_text = token_text.lower()\n","\n","    # skip stopwords and NON alphanumeric\n","    if token_text in stopwords or regexp_alphbetic.search(token_text):\n","      continue\n","\n","    sentence_tokens.append(token_text)\n","\n","  return sentence_tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1193963,"status":"ok","timestamp":1700425927806,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"},"user_tz":-60},"id":"SvVzRDA5o5If","outputId":"34419d27-82ea-4363-bb53-483d6b9a580a"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|█████████▉| 499999/500000 [19:43<00:00, 422.44it/s]\n"]}],"source":["# Remove common words and tokenize\n","word_texts = []\n","stop_at = 500000\n","\n","with open(mosaico, 'r') as file:\n","  for count, line in enumerate(tqdm(file,  total=stop_at, leave=True)):\n","    try:\n","      json_line = json.loads(line)\n","      sentence = json_line['text']\n","\n","      text_token = preprocess_word_text(sentence, stop_words, lemmatize=True)\n","      word_texts.append(text_token)\n","\n","    except json.JSONDecodeError as e:\n","      print(f\"Error decoding JSON: {str(e)}\")\n","\n","    # Stop condiction\n","    if count+1 == stop_at : break\n","\n","# remove words that appear only once\n","frequency = defaultdict(int)\n","\n","for text in word_texts:\n","  for token in text:\n","    frequency[token] += 1\n","\n","word_texts = [[token for token in text if frequency[token] > 1] for text in word_texts]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2331,"status":"ok","timestamp":1700425930397,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"},"user_tz":-60},"id":"kF_4avu4eZkk","outputId":"76a50b4a-e282-4758-d136-f8d4c9c9ebbd"},"outputs":[{"name":"stderr","output_type":"stream","text":["IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"]}],"source":["print(word_texts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_AVrEEOSuFHZ"},"outputs":[],"source":["backup_preprocessed_data(word_texts, 'non_semantic.txt')"]},{"cell_type":"markdown","metadata":{"id":"KyIIk-yLx2yH"},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djLPdBa4q1fw"},"outputs":[],"source":["# Get model\n","word_model = gensim.models.Word2Vec(word_texts, vector_size=100, window=2, epochs=20, min_count=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1700426296788,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"},"user_tz":-60},"id":"DpICNTlTrCPO","outputId":"f970a0d9-5127-4626-a76e-21b4b58af996"},"outputs":[{"data":{"text/plain":["[('bag', 0.5282819271087646),\n"," ('cutout', 0.5128922462463379),\n"," ('button', 0.5043959021568298),\n"," ('desk', 0.4992261528968811),\n"," ('tray', 0.4921962022781372),\n"," ('screen', 0.48446744680404663),\n"," ('container', 0.4827355444431305),\n"," ('jar', 0.48166587948799133),\n"," ('kiosk', 0.46833348274230957),\n"," ('sofa', 0.4659251868724823)]"]},"execution_count":221,"metadata":{},"output_type":"execute_result"}],"source":["# Inference\n","word_model.wv.most_similar('box')"]},{"cell_type":"markdown","metadata":{"id":"Tz9zZbzrf9Ot"},"source":["### Pretrained Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yN_bI1kkf8nB"},"outputs":[],"source":["# import gensim api to download pretarind model\n","import gensim.downloader as api"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44665,"status":"ok","timestamp":1700426341437,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"},"user_tz":-60},"id":"hVQC8uATgNsz","outputId":"502c2bd6-b43a-4f2c-c229-25f5f3b62bcd"},"outputs":[{"name":"stdout","output_type":"stream","text":["[==================================================] 100.0% 66.0/66.0MB downloaded\n"]},{"data":{"text/plain":["<gensim.models.keyedvectors.KeyedVectors at 0x79245a9cc8e0>"]},"execution_count":223,"metadata":{},"output_type":"execute_result"}],"source":["# Get gensim pretarined model\n","model_pretrained = api.load(\"glove-wiki-gigaword-50\")\n","model_pretrained"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1700426341438,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"},"user_tz":-60},"id":"snBLGu7sgfsv","outputId":"ff03fe40-da43-487e-ff1d-c2bb69d5f015"},"outputs":[{"data":{"text/plain":["[('boxes', 0.7834983468055725),\n"," ('piece', 0.7448906898498535),\n"," ('spot', 0.7257857322692871),\n"," ('filled', 0.7160665392875671),\n"," ('screen', 0.7088646292686462),\n"," ('onto', 0.7080516219139099),\n"," ('blank', 0.706522524356842),\n"," ('card', 0.7034342288970947),\n"," ('copy', 0.6995376348495483),\n"," ('empty', 0.6988034844398499)]"]},"execution_count":224,"metadata":{},"output_type":"execute_result"}],"source":["# Inference\n","model_pretrained.most_similar('box')"]},{"cell_type":"markdown","metadata":{"id":"T0KdChyFy9UY"},"source":["### SimLex999 evaluation"]},{"cell_type":"markdown","metadata":{"id":"v4S49Poogu9I"},"source":["#### Compute correlation between human scores and word2vec similarities\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4aUo6g-Vw2u"},"outputs":[],"source":["def compute_correlation_score(model, word_pair2score, print_warning=True, save_data=False):\n","  human_scores = []\n","  system_scores = []\n","\n","  if save_data: open('non_semantic.tsv', 'w').close() # clear output file\n","\n","  for (w1, w2), score in tqdm(word_pair2score.items(),  total=len(word_pair2score.items()), leave=True):\n","    if (w1 not in model) or (w2 not in model):\n","      system_scores.append(-1)\n","      human_scores.append(score)\n","\n","      if print_warning:\n","        print(f\"WARNING ({w1} and {w2}) are not present in the embedding model!!\" )\n","\n","      continue\n","\n","    system_similarity = model.similarity(w1, w2)\n","    human_scores.append(score)\n","    system_scores.append(system_similarity)\n","\n","    if save_data:\n","      with open('non_semantic.tsv', 'a') as target_output:\n","        target_output.write(w1 + '\\t' + w2 + '\\t' + str(system_similarity) + '\\n')\n","\n","  human_scores = np.array(human_scores)\n","  system_scores = np.array(system_scores)\n","\n","  pearson_r, _ = scipy.stats.pearsonr(human_scores, system_scores)    # Pearson's r\n","  spearman_rho = scipy.stats.spearmanr(human_scores, system_scores).statistic   # Spearman's rho\n","\n","  return pearson_r, spearman_rho"]},{"cell_type":"markdown","metadata":{"id":"CE3FCEn7h0B2"},"source":["#### Performances"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":423,"status":"ok","timestamp":1700426341843,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"},"user_tz":-60},"id":"SH9BO-kGhhEU","outputId":"0e68f6e7-9aa8-48fc-f05e-b98e2a5f636e"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 999/999 [00:00<00:00, 16575.98it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING (do and quit) are not present in the embedding model!!\n","WARNING (do and happen) are not present in the embedding model!!\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/plain":["(0.41546162329260283, 0.4145001988694379)"]},"execution_count":226,"metadata":{},"output_type":"execute_result"}],"source":["# word2vect\n","compute_correlation_score(word_model.wv, simplex_pairs, print_warning=True, save_data=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1700426341844,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"},"user_tz":-60},"id":"VqxdClGWhqR7","outputId":"e516ebf8-b8b1-4264-c469-e1166e05e5c7"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 999/999 [00:00<00:00, 60955.03it/s]\n"]},{"data":{"text/plain":["(0.2941386830730656, 0.2645792192990813)"]},"execution_count":227,"metadata":{},"output_type":"execute_result"}],"source":["# Pretarined gensim model\n","compute_correlation_score(model_pretrained, simplex_pairs)"]},{"cell_type":"markdown","metadata":{"id":"I950Db2Iyeus"},"source":["## Sense Embedding\n"]},{"cell_type":"markdown","metadata":{"id":"IWpNkvdzyr-B"},"source":["### Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T8FwdmWlyq2F"},"outputs":[],"source":["# Process text and substitute word with sense\n","# nlp = spacy.load(\"en_core_web_sm\")\n","regexp_alphbetic = re.compile('[^a-zA-Z]+')\n","lemmatizer = WordNetLemmatizer()\n","\n","def preprocess_sense_text(sentence, annotations, stopwords, lemmatize=True):\n","\n","  doc = sentence.split(' ')\n","  pos = nltk.pos_tag(doc)\n","\n","  sentence_tokens = []\n","  line_word_sense = dict()\n","\n","  # get the mapping from some word to the corresponding sense\n","  # to disanbiguate the text\n","  for i in range(len(annotations)):\n","    index = annotations[i]['token_span'][0]\n","    sense = annotations[i]['label']\n","    line_word_sense[index] = sense\n","\n","\n","  for token_id, token in enumerate(doc):\n","\n","    # tokenization\n","    token_text = lemmatizer.lemmatize(token, get_wordnet_pos(pos[token_id])) if lemmatize else token.text\n","    token_text = token_text.lower()\n","\n","    if token_id in line_word_sense.keys():\n","      sense = line_word_sense[token_id]\n","\n","      if token_text in sense[:sense.index('%')]:\n","        sentence_tokens.append(line_word_sense[token_id])\n","\n","    # skip stopwords and NON alphanumeric\n","    elif not ((token_text in stopwords) or regexp_alphbetic.search(token_text)):\n","      sentence_tokens.append(token_text)\n","\n","    else:\n","      continue\n","\n","  return sentence_tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"bN7-okhppQtl","outputId":"75ffdb93-59ab-42fc-e16c-b84e81292a69"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|█████████▉| 499999/500000 [18:29<00:00, 450.62it/s]\n"]}],"source":["# Extract information from dataset\n","stop_at = 500000\n","sense_texts = [] # unambiguous document\n","\n","with open(mosaico, 'r') as file:\n","  for count, line in enumerate(tqdm(file,  total=stop_at, leave=True)):\n","    try:\n","      json_line = json.loads(line)\n","      sentence = json_line['text']\n","      annotation = json_line['annotations']\n","\n","      text_token = preprocess_sense_text(sentence, annotation, stop_words, lemmatize=True)\n","      sense_texts.append(text_token)\n","\n","    except json.JSONDecodeError as e:\n","      print(f\"Error decoding JSON: {str(e)}\")\n","\n","    # Stop condiction\n","    if count+1 == stop_at : break\n","\n","# remove words that appear only once\n","frequency = defaultdict(int)\n","for text in sense_texts:\n","  for token in text:\n","    frequency[token] += 1\n","\n","sense_texts = [[token for token in text if frequency[token] > 0] for text in sense_texts]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EnxTWt7vxkW6","outputId":"a25f751b-5b1c-41ce-ce6c-3eaf2bf65356"},"outputs":[{"name":"stderr","output_type":"stream","text":["IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"]}],"source":["print(sense_texts)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qoBfpG3QxoN8"},"outputs":[],"source":["backup_preprocessed_data(sense_texts, 'semantic.txt')"]},{"cell_type":"markdown","metadata":{"id":"IhkdOSsroTHT"},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ojXdFShVQfoE"},"outputs":[],"source":["# restor backup\n","sense_texts = []\n","\n","with open('semantic.txt', 'r') as file:\n","  for line in file:\n","    value = line.split(' ')\n","    sense_texts.append(value)\n","    count += 1\n","\n","    if count == stop_at:  break"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"N7DT8tyYoVHV"},"outputs":[],"source":["# Get model\n","sense_model = gensim.models.Word2Vec(sense_texts, vector_size=100, window=2, epochs=20, min_count=1)"]},{"cell_type":"markdown","metadata":{"id":"_ZzNNSvfcPQ0"},"source":["### SimLex999 semantic evaluation"]},{"cell_type":"markdown","metadata":{"id":"eTlPblutcckz"},"source":["#### Compute correlation between human scores and word2vec semantic similarities"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LFQ5zPD6moVq"},"outputs":[],"source":["def compute_semantic_correlation_score(model, simlex_sense_pairs,\n","                                       print_warning=True, save_data=False):\n","  human_scores = []\n","  system_scores = []\n","\n","  if save_data: open('semantic.tsv', 'w').close() # clear output file\n","\n","  for (senses_1, senses_2), score in simlex_sense_pairs.items():\n","    senses_1 = senses_1.split(',')\n","    senses_2 = senses_2.split(',')\n","    senses_1_in_model = [s for s in senses_1 if s in model]\n","    senses_2_in_model = [s for s in senses_2 if s in model]\n","\n","\n","    if len(senses_1_in_model) == 0 or len(senses_2_in_model) == 0:\n","\n","      # sense is not present in the model\n","      s1_str = \" \".join(senses_1)\n","      s2_str = \" \".join(senses_2)\n","\n","      if print_warning:\n","        print(f\"WARNING ({s1_str} and {s2_str}) are not present in the embedding model!!\" )\n","\n","      system_scores.append(-1)\n","      human_scores.append(float(score))\n","\n","      continue\n","\n","    else:\n","      all_similarities = []\n","\n","      for s1 in senses_1_in_model:\n","        for s2 in senses_2_in_model:\n","          all_similarities.append(model.similarity(s1, s2))\n","\n","      system_similarity = max(all_similarities)\n","\n","      if save_data:\n","        with open('semantic.tsv','a') as output:\n","          word_tag_1 = senses_1[0][:senses_1[0].index('%')]\n","          word_tag_2 = senses_2[0][:senses_2[0].index('%')]\n","          output.write(word_tag_1 + '\\t' + word_tag_1 + '\\t' + str(system_similarity) + '\\n')\n","\n","      human_scores.append(float(score))\n","      system_scores.append(system_similarity)\n","\n","  human_scores = np.array(human_scores)\n","  system_scores = np.array(system_scores)\n","\n","  # Calculate Pearson's r (Pearson correlation coefficient) and\n","  # Spearman's rho (Spearman rank correlation coefficient)\n","  pearson_r, _ = scipy.stats.pearsonr(human_scores, system_scores)    # Pearson's r\n","  spearman_rho = scipy.stats.spearmanr(human_scores, system_scores).statistic   # Spearman's rho\n","\n","  return pearson_r, spearman_rho"]},{"cell_type":"markdown","metadata":{"id":"hUc8q-ZtqPVp"},"source":["#### Performances"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"194u0JRxqIzO","outputId":"35c44926-984e-4927-df2a-16ded283f2d8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n","  warnings.warn(stats.ConstantInputWarning(msg))\n","/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n","  warnings.warn(stats.ConstantInputWarning(warn_msg))\n"]},{"data":{"text/plain":["(nan, nan)"]},"execution_count":235,"metadata":{},"output_type":"execute_result"}],"source":["compute_semantic_correlation_score(sense_model.wv, simlex_sense_pairs, print_warning=False, save_data=True)"]},{"cell_type":"markdown","metadata":{"id":"Txtj5mimln8i"},"source":["## Explicit Representation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1ANwtQwUzUjTLlWKXz77Y-VO4uuq9Z54y"},"id":"lveMAEXKCjZO","outputId":"9e6ee670-f802-492b-9ff7-e5b9a5552b8a"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["vocabulary = set()\n","stop_at = 10000\n","count = 0\n","reduced = []\n","\n","with open('semantic.txt', 'r') as file:\n","  for line in file:\n","    value = line.split(' ')\n","    reduced.append(value)\n","    new_set = set(value)\n","    vocabulary = vocabulary.union(new_set)\n","    count += 1\n","    if count == stop_at:  break\n","\n","vocabulary = list(vocabulary)\n","print(vocabulary)\n","print(reduced)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iSzQu4aalvpX"},"outputs":[],"source":["explicit = {word_key: {} for word_key in vocabulary}\n","\n","for lines in reduced:\n","  for indx, word in enumerate(lines):\n","    for window in [-2,-1,1,2]:\n","      new_indx = indx+window\n","\n","      if 0 <= new_indx < len(lines):\n","        compare_word = lines[new_indx]\n","        voc_indx = vocabulary.index(compare_word)\n","\n","        if voc_indx in explicit[word].keys():\n","          explicit[word][voc_indx] += 1\n","\n","        else:\n","          explicit[word][voc_indx] = 1\n","\n","print(explicit)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EXK3p1SeDg5U"},"outputs":[],"source":["# cosine similarity function\n","def cosine_similarity(w1, w2):\n","\n","  common_keys = set(w1.keys()).intersection(w2.keys())\n","  dot_product = 0\n","  norm1 = 0\n","  norm2 = 0\n","\n","  for key in common_keys:\n","    norm1 += w1[key]**2\n","    norm2 += w2[key]**2\n","    dot_product = w1[key]*w2[key]\n","\n","  norm_product = (math.sqrt(norm1) * math.sqrt(norm2))\n","\n","  cosine_sim = dot_product/norm_product if norm_product != 0 else 0\n","  return cosine_sim"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4y9tkKAMsPBK"},"outputs":[],"source":["def compute__explicit_semantic_correlation_score(explicit, simlex_sense_pairs,\n","                                       print_warning=True, save_data=False):\n","  system_scores = []\n","  human_scores = []\n","  for (senses_1, senses_2), score in simlex_sense_pairs.items():\n","\n","    all_similarities = []\n","\n","    for s1 in senses_1:\n","      for s2 in senses_2:\n","        if s1 in explicit.keys() and s2 in explicit.keys():\n","          w1 = explicit[s1]\n","          w2 = explicit[s2]\n","          cosine_sim = cosine_similarity(w1, w2)\n","          all_similarities.append(cosine_sim)\n","\n","        else:\n","          all_similarities.append(-1)\n","\n","    system_similarity = max(all_similarities)\n","    system_scores.append(system_similarity)\n","    human_scores.append(float(score))\n","\n","  # Calculate Pearson's r (Pearson correlation coefficient)\n","  # Spearman's rho (Spearman rank correlation coefficient)\n","  pearson_r, _ = scipy.stats.pearsonr(human_scores, system_scores)    # Pearson's r\n","  spearman_rho = scipy.stats.spearmanr(human_scores, system_scores).statistic   # Spearman's rho\n","\n","  return pearson_r,spearman_rho"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"So58L5BjtPX4","outputId":"d56cdb37-4c2b-4e92-bbb4-e57659eff931"},"outputs":[{"data":{"text/plain":["(0.017800447597569845, 0.019522777579842776)"]},"execution_count":240,"metadata":{},"output_type":"execute_result"}],"source":["compute__explicit_semantic_correlation_score(explicit, simlex_sense_pairs,print_warning=True, save_data=False)"]},{"cell_type":"markdown","metadata":{"id":"pMiNNAYG_-CG"},"source":["## Personalized Page Rank"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6CFlDTOA_8Ab"},"outputs":[],"source":["# TODO\n","# compute sense/word PPMI\n","# get beack sense to sense PPMI\n","# populate the graph with this"]}],"metadata":{"colab":{"collapsed_sections":["5-9yYVjSxFdv","U49GmfvPeacu","IWpNkvdzyr-B","IhkdOSsroTHT","eTlPblutcckz","Txtj5mimln8i"],"provenance":[],"authorship_tag":"ABX9TyOxNtvizeRE4yVIzbevBR9L"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}